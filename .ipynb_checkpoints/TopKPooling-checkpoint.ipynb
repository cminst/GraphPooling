{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/outdated/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 153 for seed 42\n",
      "Early stopping at epoch 176 for seed 43\n",
      "Early stopping at epoch 153 for seed 44\n",
      "Average Time: 4.81 seconds\n",
      "Var Time: 0.43 seconds\n",
      "Average Memory: 92.00 MB\n",
      "Average Best Val Acc: 0.8214\n",
      "Std Best Test Acc: 0.0488\n",
      "Average Test Acc: 0.8276\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 150\n",
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"MUTAG\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class HierarchicalGCN_TOPK(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_TOPK, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = TopKPooling(hidden_channels, ratio=0.9)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = TopKPooling(hidden_channels, ratio=0.9)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 1.1108, Val Acc: 0.3213, Test Acc: 0.3320\n",
      "Seed: 42, Epoch: 002, Loss: 1.1024, Val Acc: 0.3213, Test Acc: 0.3320\n",
      "Seed: 42, Epoch: 003, Loss: 1.0940, Val Acc: 0.3213, Test Acc: 0.3320\n",
      "Seed: 42, Epoch: 004, Loss: 1.0858, Val Acc: 0.5427, Test Acc: 0.5560\n",
      "Seed: 42, Epoch: 005, Loss: 1.0779, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 006, Loss: 1.0686, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 007, Loss: 1.0554, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 008, Loss: 1.0319, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 009, Loss: 0.9899, Val Acc: 0.5413, Test Acc: 0.5613\n",
      "Seed: 42, Epoch: 010, Loss: 0.9371, Val Acc: 0.4747, Test Acc: 0.5080\n",
      "Seed: 42, Epoch: 011, Loss: 0.8996, Val Acc: 0.4893, Test Acc: 0.5040\n",
      "Seed: 42, Epoch: 012, Loss: 0.8724, Val Acc: 0.5547, Test Acc: 0.5760\n",
      "Seed: 42, Epoch: 013, Loss: 0.8435, Val Acc: 0.5827, Test Acc: 0.5840\n",
      "Seed: 42, Epoch: 014, Loss: 0.8179, Val Acc: 0.6067, Test Acc: 0.5893\n",
      "Seed: 42, Epoch: 015, Loss: 0.7960, Val Acc: 0.6200, Test Acc: 0.5960\n",
      "Seed: 42, Epoch: 016, Loss: 0.7758, Val Acc: 0.6307, Test Acc: 0.6093\n",
      "Seed: 42, Epoch: 017, Loss: 0.7561, Val Acc: 0.6440, Test Acc: 0.6333\n",
      "Seed: 42, Epoch: 018, Loss: 0.7349, Val Acc: 0.6507, Test Acc: 0.6373\n",
      "Seed: 42, Epoch: 019, Loss: 0.7182, Val Acc: 0.6653, Test Acc: 0.6573\n",
      "Seed: 42, Epoch: 020, Loss: 0.7022, Val Acc: 0.6653, Test Acc: 0.6453\n",
      "Seed: 42, Epoch: 021, Loss: 0.6876, Val Acc: 0.6693, Test Acc: 0.6600\n",
      "Seed: 42, Epoch: 022, Loss: 0.6767, Val Acc: 0.6640, Test Acc: 0.6587\n",
      "Seed: 42, Epoch: 023, Loss: 0.6636, Val Acc: 0.6653, Test Acc: 0.6653\n",
      "Seed: 42, Epoch: 024, Loss: 0.6532, Val Acc: 0.6653, Test Acc: 0.6653\n",
      "Seed: 42, Epoch: 025, Loss: 0.6414, Val Acc: 0.6653, Test Acc: 0.6680\n",
      "Seed: 42, Epoch: 026, Loss: 0.6355, Val Acc: 0.6733, Test Acc: 0.6707\n",
      "Seed: 42, Epoch: 027, Loss: 0.6268, Val Acc: 0.6720, Test Acc: 0.6680\n",
      "Seed: 42, Epoch: 028, Loss: 0.6210, Val Acc: 0.6667, Test Acc: 0.6693\n",
      "Seed: 42, Epoch: 029, Loss: 0.6130, Val Acc: 0.6747, Test Acc: 0.6680\n",
      "Seed: 42, Epoch: 030, Loss: 0.6078, Val Acc: 0.6733, Test Acc: 0.6707\n",
      "Seed: 42, Epoch: 031, Loss: 0.6007, Val Acc: 0.6773, Test Acc: 0.6747\n",
      "Seed: 42, Epoch: 032, Loss: 0.5946, Val Acc: 0.6840, Test Acc: 0.6760\n",
      "Seed: 42, Epoch: 033, Loss: 0.5875, Val Acc: 0.6760, Test Acc: 0.6760\n",
      "Seed: 42, Epoch: 034, Loss: 0.5851, Val Acc: 0.6920, Test Acc: 0.6733\n",
      "Seed: 42, Epoch: 035, Loss: 0.5804, Val Acc: 0.6747, Test Acc: 0.6733\n",
      "Seed: 42, Epoch: 036, Loss: 0.5744, Val Acc: 0.6947, Test Acc: 0.6773\n",
      "Seed: 42, Epoch: 037, Loss: 0.5715, Val Acc: 0.6987, Test Acc: 0.6707\n",
      "Seed: 42, Epoch: 038, Loss: 0.5633, Val Acc: 0.6800, Test Acc: 0.6720\n",
      "Seed: 42, Epoch: 039, Loss: 0.5597, Val Acc: 0.6960, Test Acc: 0.6733\n",
      "Seed: 42, Epoch: 040, Loss: 0.5539, Val Acc: 0.6853, Test Acc: 0.6800\n",
      "Seed: 42, Epoch: 041, Loss: 0.5506, Val Acc: 0.7013, Test Acc: 0.6840\n",
      "Seed: 42, Epoch: 042, Loss: 0.5502, Val Acc: 0.7080, Test Acc: 0.6867\n",
      "Seed: 42, Epoch: 043, Loss: 0.5408, Val Acc: 0.7080, Test Acc: 0.6907\n",
      "Seed: 42, Epoch: 044, Loss: 0.5384, Val Acc: 0.7107, Test Acc: 0.6720\n",
      "Seed: 42, Epoch: 045, Loss: 0.5355, Val Acc: 0.7387, Test Acc: 0.6973\n",
      "Seed: 42, Epoch: 046, Loss: 0.5325, Val Acc: 0.7453, Test Acc: 0.6947\n",
      "Seed: 42, Epoch: 047, Loss: 0.5309, Val Acc: 0.7453, Test Acc: 0.7093\n",
      "Seed: 42, Epoch: 048, Loss: 0.5260, Val Acc: 0.7560, Test Acc: 0.7053\n",
      "Seed: 42, Epoch: 049, Loss: 0.5257, Val Acc: 0.7573, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 050, Loss: 0.5272, Val Acc: 0.7560, Test Acc: 0.7107\n",
      "Seed: 42, Epoch: 051, Loss: 0.5232, Val Acc: 0.7440, Test Acc: 0.7053\n",
      "Seed: 42, Epoch: 052, Loss: 0.5232, Val Acc: 0.7600, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 053, Loss: 0.5178, Val Acc: 0.7613, Test Acc: 0.7107\n",
      "Seed: 42, Epoch: 054, Loss: 0.5120, Val Acc: 0.7653, Test Acc: 0.7093\n",
      "Seed: 42, Epoch: 055, Loss: 0.5130, Val Acc: 0.7533, Test Acc: 0.7147\n",
      "Seed: 42, Epoch: 056, Loss: 0.5165, Val Acc: 0.7493, Test Acc: 0.7120\n",
      "Seed: 42, Epoch: 057, Loss: 0.5159, Val Acc: 0.7533, Test Acc: 0.7173\n",
      "Seed: 42, Epoch: 058, Loss: 0.5090, Val Acc: 0.7613, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 059, Loss: 0.5090, Val Acc: 0.7560, Test Acc: 0.7213\n",
      "Seed: 42, Epoch: 060, Loss: 0.5044, Val Acc: 0.7613, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 061, Loss: 0.5049, Val Acc: 0.7627, Test Acc: 0.7240\n",
      "Seed: 42, Epoch: 062, Loss: 0.5042, Val Acc: 0.7547, Test Acc: 0.7240\n",
      "Seed: 42, Epoch: 063, Loss: 0.5002, Val Acc: 0.7573, Test Acc: 0.7227\n",
      "Seed: 42, Epoch: 064, Loss: 0.4996, Val Acc: 0.7613, Test Acc: 0.7307\n",
      "Seed: 42, Epoch: 065, Loss: 0.4984, Val Acc: 0.7573, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 066, Loss: 0.5007, Val Acc: 0.7733, Test Acc: 0.7240\n",
      "Seed: 42, Epoch: 067, Loss: 0.4978, Val Acc: 0.7573, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 068, Loss: 0.4949, Val Acc: 0.7693, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 069, Loss: 0.4951, Val Acc: 0.7613, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 070, Loss: 0.4952, Val Acc: 0.7693, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 071, Loss: 0.4930, Val Acc: 0.7653, Test Acc: 0.7213\n",
      "Seed: 42, Epoch: 072, Loss: 0.4920, Val Acc: 0.7707, Test Acc: 0.7293\n",
      "Seed: 42, Epoch: 073, Loss: 0.4903, Val Acc: 0.7720, Test Acc: 0.7213\n",
      "Seed: 42, Epoch: 074, Loss: 0.4892, Val Acc: 0.7720, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 075, Loss: 0.4868, Val Acc: 0.7667, Test Acc: 0.7293\n",
      "Seed: 42, Epoch: 076, Loss: 0.4862, Val Acc: 0.7773, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 077, Loss: 0.4853, Val Acc: 0.7587, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 078, Loss: 0.4832, Val Acc: 0.7760, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 079, Loss: 0.4826, Val Acc: 0.7773, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 080, Loss: 0.4821, Val Acc: 0.7760, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 081, Loss: 0.4803, Val Acc: 0.7787, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 082, Loss: 0.4819, Val Acc: 0.7693, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 083, Loss: 0.4761, Val Acc: 0.7787, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 084, Loss: 0.4758, Val Acc: 0.7773, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 085, Loss: 0.4742, Val Acc: 0.7733, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 086, Loss: 0.4746, Val Acc: 0.7733, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 087, Loss: 0.4728, Val Acc: 0.7813, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 088, Loss: 0.4720, Val Acc: 0.7787, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 089, Loss: 0.4699, Val Acc: 0.7760, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 090, Loss: 0.4701, Val Acc: 0.7840, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 091, Loss: 0.4676, Val Acc: 0.7747, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 092, Loss: 0.4674, Val Acc: 0.7720, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 093, Loss: 0.4669, Val Acc: 0.7760, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 094, Loss: 0.4637, Val Acc: 0.7800, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 095, Loss: 0.4632, Val Acc: 0.7800, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 096, Loss: 0.4608, Val Acc: 0.7653, Test Acc: 0.7560\n",
      "Seed: 42, Epoch: 097, Loss: 0.4582, Val Acc: 0.7693, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 098, Loss: 0.4522, Val Acc: 0.7827, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 099, Loss: 0.4516, Val Acc: 0.7680, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 100, Loss: 0.4505, Val Acc: 0.7507, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 101, Loss: 0.4493, Val Acc: 0.7613, Test Acc: 0.7520\n",
      "Seed: 42, Epoch: 102, Loss: 0.4444, Val Acc: 0.7587, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 103, Loss: 0.4496, Val Acc: 0.7760, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 104, Loss: 0.4506, Val Acc: 0.7693, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 105, Loss: 0.4466, Val Acc: 0.7827, Test Acc: 0.7520\n",
      "Seed: 42, Epoch: 106, Loss: 0.4448, Val Acc: 0.7587, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 107, Loss: 0.4491, Val Acc: 0.7653, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 108, Loss: 0.4446, Val Acc: 0.7613, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 109, Loss: 0.4421, Val Acc: 0.7720, Test Acc: 0.7560\n",
      "Seed: 42, Epoch: 110, Loss: 0.4352, Val Acc: 0.7680, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 111, Loss: 0.4381, Val Acc: 0.7813, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 112, Loss: 0.4352, Val Acc: 0.7800, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 113, Loss: 0.4332, Val Acc: 0.7653, Test Acc: 0.7560\n",
      "Seed: 42, Epoch: 114, Loss: 0.4276, Val Acc: 0.7787, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 115, Loss: 0.4290, Val Acc: 0.7693, Test Acc: 0.7520\n",
      "Seed: 42, Epoch: 116, Loss: 0.4302, Val Acc: 0.7707, Test Acc: 0.7627\n",
      "Seed: 42, Epoch: 117, Loss: 0.4276, Val Acc: 0.7813, Test Acc: 0.7573\n",
      "Seed: 42, Epoch: 118, Loss: 0.4283, Val Acc: 0.7853, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 119, Loss: 0.4284, Val Acc: 0.7680, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 120, Loss: 0.4231, Val Acc: 0.7733, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 121, Loss: 0.4218, Val Acc: 0.7760, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 122, Loss: 0.4228, Val Acc: 0.7773, Test Acc: 0.7627\n",
      "Seed: 42, Epoch: 123, Loss: 0.4183, Val Acc: 0.7760, Test Acc: 0.7627\n",
      "Seed: 42, Epoch: 124, Loss: 0.4216, Val Acc: 0.7707, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 125, Loss: 0.4175, Val Acc: 0.7667, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 126, Loss: 0.4172, Val Acc: 0.7733, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 127, Loss: 0.4152, Val Acc: 0.7733, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 128, Loss: 0.4136, Val Acc: 0.7760, Test Acc: 0.7627\n",
      "Seed: 42, Epoch: 129, Loss: 0.4157, Val Acc: 0.7720, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 130, Loss: 0.4213, Val Acc: 0.7733, Test Acc: 0.7640\n"
     ]
    }
   ],
   "source": [
    "max_nodes = 500\n",
    "data_path = \"./data\"\n",
    "\n",
    "dataset_sparse = TUDataset(\n",
    "    root=data_path,\n",
    "    name=\"COLLAB\",\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes,\n",
    "    transform=T.Compose([\n",
    "        T.ToUndirected(),\n",
    "        T.OneHotDegree(491),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class HierarchicalGCN_TOPK(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_TOPK, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = TopKPooling(hidden_channels, ratio=0.9)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = TopKPooling(hidden_channels, ratio=0.9)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"DD\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class HierarchicalGCN_TOPK(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_TOPK, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = TopKPooling(hidden_channels, ratio=0.9)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = TopKPooling(hidden_channels, ratio=0.9)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-MULTI\", transform=T.Compose([T.OneHotDegree(88)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class HierarchicalGCN_TOPK(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_TOPK, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = TopKPooling(hidden_channels, ratio=0.9)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = TopKPooling(hidden_channels, ratio=0.9)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "data_path = \"/data1/Pooling/\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"COLLAB\", transform=T.Compose([T.OneHotDegree(491)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class HierarchicalGCN_TOPK(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_TOPK, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = TopKPooling(hidden_channels, ratio=0.7)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = TopKPooling(hidden_channels, ratio=0.7)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --topk_ratio=0.1 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --topk_ratio=0.3 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --topk_ratio=0.5 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --topk_ratio=0.7 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --topk_ratio=0.9 --pooling='TopK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --topk_ratio=0.1 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --topk_ratio=0.3 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --topk_ratio=0.5 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --topk_ratio=0.7 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --topk_ratio=0.9 --pooling='TopK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.1 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.3 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.5 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.7 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.9 --pooling='TopK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.1 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.3 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.5 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.7 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.9 --pooling='TopK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freesolv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.1 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.3 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.5 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.7 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.9 --pooling='TopK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipophilicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.1 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.3 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.5 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.7 --pooling='TopK'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --topk_ratio=0.9 --pooling='TopK'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
