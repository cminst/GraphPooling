{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/outdated/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn.models.mlp import Linear\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROTEINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "max_nodes = 700\n",
    "data_path = \"./data\"\n",
    "dataset_dense = TUDataset(\n",
    "    data_path,\n",
    "    name=\"PROTEINS\",\n",
    "    transform=T.Compose([T.ToDense(max_nodes)]),\n",
    "    use_node_attr=True,\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes,\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        if lin:\n",
    "            self.lin = torch.nn.Linear(out_channels, out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "    def bn(self, i, x):\n",
    "        batch_size, num_nodes, num_channels = x.size()\n",
    "        x = x.view(-1, num_channels)\n",
    "        x = getattr(self, f'bn{i}')(x)\n",
    "        x = x.view(batch_size, num_nodes, num_channels)\n",
    "        return x\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x = self.bn(1, self.conv1(x, adj, mask).relu())\n",
    "        x = self.bn(2, self.conv2(x, adj, mask).relu())\n",
    "        x = self.bn(3, self.conv3(x, adj, mask).relu())\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "        return x\n",
    "class Net_Diff(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_nodes = 64\n",
    "        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = DenseGCNConv(dataset_dense.num_features, 64)\n",
    "        num_nodes = 64\n",
    "        self.gnn2_pool = GNN(64, 64, num_nodes)\n",
    "        self.gnn2_embed = DenseGCNConv(64, 64)\n",
    "        self.gnn3_embed = DenseGCNConv(64, 64)\n",
    "        self.lin1 = torch.nn.Linear(64, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, dataset_dense.num_classes)\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = Net_Diff().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_dense = dataset_dense.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_dense)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_dense[:num_train]\n",
    "    val_dataset = dataset_dense[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_dense[num_train + num_val:]\n",
    "    train_loader = DenseDataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DenseDataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DenseDataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = Net_Diff().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCI1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "max_nodes = 150\n",
    "data_path = \"./data\"\n",
    "dataset_dense = TUDataset(\n",
    "    data_path,\n",
    "    name=\"NCI1\",\n",
    "    transform=T.Compose([T.ToDense(max_nodes)]),\n",
    "    use_node_attr=True,\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes,\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        if lin:\n",
    "            self.lin = torch.nn.Linear(out_channels, out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "    def bn(self, i, x):\n",
    "        batch_size, num_nodes, num_channels = x.size()\n",
    "        x = x.view(-1, num_channels)\n",
    "        x = getattr(self, f'bn{i}')(x)\n",
    "        x = x.view(batch_size, num_nodes, num_channels)\n",
    "        return x\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x = self.bn(1, self.conv1(x, adj, mask).relu())\n",
    "        x = self.bn(2, self.conv2(x, adj, mask).relu())\n",
    "        x = self.bn(3, self.conv3(x, adj, mask).relu())\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "        return x\n",
    "class Net_Diff(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_nodes = 64\n",
    "        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = DenseGCNConv(dataset_dense.num_features, 64)\n",
    "        num_nodes = 64\n",
    "        self.gnn2_pool = GNN(64, 64, num_nodes)\n",
    "        self.gnn2_embed = DenseGCNConv(64, 64)\n",
    "        self.gnn3_embed = DenseGCNConv(64, 64)\n",
    "        self.lin1 = torch.nn.Linear(64, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, dataset_dense.num_classes)\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = Net_Diff().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_dense = dataset_dense.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_dense)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_dense[:num_train]\n",
    "    val_dataset = dataset_dense[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_dense[num_train + num_val:]\n",
    "    train_loader = DenseDataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DenseDataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DenseDataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = Net_Diff().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCI109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "max_nodes = 150\n",
    "data_path = \"./data\"\n",
    "dataset_dense = TUDataset(\n",
    "    data_path,\n",
    "    name=\"NCI109\",\n",
    "    transform=T.Compose([T.ToDense(max_nodes)]),\n",
    "    use_node_attr=True,\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes,\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        if lin:\n",
    "            self.lin = torch.nn.Linear(out_channels, out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "    def bn(self, i, x):\n",
    "        batch_size, num_nodes, num_channels = x.size()\n",
    "        x = x.view(-1, num_channels)\n",
    "        x = getattr(self, f'bn{i}')(x)\n",
    "        x = x.view(batch_size, num_nodes, num_channels)\n",
    "        return x\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x = self.bn(1, self.conv1(x, adj, mask).relu())\n",
    "        x = self.bn(2, self.conv2(x, adj, mask).relu())\n",
    "        x = self.bn(3, self.conv3(x, adj, mask).relu())\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "        return x\n",
    "class Net_Diff(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_nodes = 64\n",
    "        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = DenseGCNConv(dataset_dense.num_features, 64)\n",
    "        num_nodes = 64\n",
    "        self.gnn2_pool = GNN(64, 64, num_nodes)\n",
    "        self.gnn2_embed = DenseGCNConv(64, 64)\n",
    "        self.gnn3_embed = DenseGCNConv(64, 64)\n",
    "        self.lin1 = torch.nn.Linear(64, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, dataset_dense.num_classes)\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = Net_Diff().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_dense = dataset_dense.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_dense)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_dense[:num_train]\n",
    "    val_dataset = dataset_dense[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_dense[num_train + num_val:]\n",
    "    train_loader = DenseDataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DenseDataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DenseDataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = Net_Diff().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "data_path = \"./data\"\n",
    "max_nodes = 150\n",
    "class ConvertToDenseAdj(BaseTransform):\n",
    "    def __call__(self, data):\n",
    "        if hasattr(data, 'adj') and data.adj.dim() == 3:\n",
    "            data.adj = data.adj.sum(dim=-1)\n",
    "        return data\n",
    "dataset_dense = TUDataset(\n",
    "    data_path,\n",
    "    name=\"MUTAG\",\n",
    "    transform=T.Compose([T.ToDense(max_nodes), ConvertToDenseAdj()]),\n",
    "    use_node_attr=True,\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes,\n",
    ")\n",
    "dataset_dense[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        if lin:\n",
    "            self.lin = torch.nn.Linear(out_channels, out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "    def bn(self, i, x):\n",
    "        batch_size, num_nodes, num_channels = x.size()\n",
    "        x = x.view(-1, num_channels)\n",
    "        x = getattr(self, f'bn{i}')(x)\n",
    "        x = x.view(batch_size, num_nodes, num_channels)\n",
    "        return x\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x = self.bn(1, self.conv1(x, adj, mask).relu())\n",
    "        x = self.bn(2, self.conv2(x, adj, mask).relu())\n",
    "        x = self.bn(3, self.conv3(x, adj, mask).relu())\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "        return x\n",
    "class Net_Diff(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_nodes = 64\n",
    "        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = DenseGCNConv(dataset_dense.num_features, 64)\n",
    "        num_nodes = 64\n",
    "        self.gnn2_pool = GNN(64, 64, num_nodes)\n",
    "        self.gnn2_embed = DenseGCNConv(64, 64)\n",
    "        self.gnn3_embed = DenseGCNConv(64, 64)\n",
    "        self.lin1 = torch.nn.Linear(64, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, dataset_dense.num_classes)\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = Net_Diff().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_dense = dataset_dense.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_dense)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_dense[:num_train]\n",
    "    val_dataset = dataset_dense[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_dense[num_train + num_val:]\n",
    "    train_loader = DenseDataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DenseDataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DenseDataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = Net_Diff().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "max_nodes = 500\n",
    "data_path = \"./data\"\n",
    "dataset_dense = TUDataset(\n",
    "    data_path,\n",
    "    name=\"DD\",\n",
    "    transform=T.Compose([T.ToDense(max_nodes)]),\n",
    "    use_node_attr=True,\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes,\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        if lin:\n",
    "            self.lin = torch.nn.Linear(out_channels, out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "    def bn(self, i, x):\n",
    "        batch_size, num_nodes, num_channels = x.size()\n",
    "        x = x.view(-1, num_channels)\n",
    "        x = getattr(self, f'bn{i}')(x)\n",
    "        x = x.view(batch_size, num_nodes, num_channels)\n",
    "        return x\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x = self.bn(1, self.conv1(x, adj, mask).relu())\n",
    "        x = self.bn(2, self.conv2(x, adj, mask).relu())\n",
    "        x = self.bn(3, self.conv3(x, adj, mask).relu())\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "        return x\n",
    "class Net_Diff(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_nodes = 64\n",
    "        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = DenseGCNConv(dataset_dense.num_features, 64)\n",
    "        num_nodes = 64\n",
    "        self.gnn2_pool = GNN(64, 64, num_nodes)\n",
    "        self.gnn2_embed = DenseGCNConv(64, 64)\n",
    "        self.gnn3_embed = DenseGCNConv(64, 64)\n",
    "        self.lin1 = torch.nn.Linear(64, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, dataset_dense.num_classes)\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = Net_Diff().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_dense = dataset_dense.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_dense)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_dense[:num_train]\n",
    "    val_dataset = dataset_dense[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_dense[num_train + num_val:]\n",
    "    train_loader = DenseDataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    valid_loader = DenseDataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "    test_loader = DenseDataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "    model = Net_Diff().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"./data\"\n",
    "dataset_dense = TUDataset(\n",
    "    data_path,\n",
    "    name=\"IMDB-BINARY\",\n",
    "    transform=T.Compose([T.OneHotDegree(136), T.ToDense(max_nodes)]),\n",
    "    use_node_attr=True,\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes,\n",
    ")\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        if lin:\n",
    "            self.lin = torch.nn.Linear(out_channels, out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "    def bn(self, i, x):\n",
    "        batch_size, num_nodes, num_channels = x.size()\n",
    "        x = x.view(-1, num_channels)\n",
    "        x = getattr(self, f'bn{i}')(x)\n",
    "        x = x.view(batch_size, num_nodes, num_channels)\n",
    "        return x\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x = self.bn(1, self.conv1(x, adj, mask).relu())\n",
    "        x = self.bn(2, self.conv2(x, adj, mask).relu())\n",
    "        x = self.bn(3, self.conv3(x, adj, mask).relu())\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "        return x\n",
    "class Net_Diff(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_nodes = 64\n",
    "        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = DenseGCNConv(dataset_dense.num_features, 64)\n",
    "        num_nodes = 64\n",
    "        self.gnn2_pool = GNN(64, 64, num_nodes)\n",
    "        self.gnn2_embed = DenseGCNConv(64, 64)\n",
    "        self.gnn3_embed = DenseGCNConv(64, 64)\n",
    "        self.lin1 = torch.nn.Linear(64, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, dataset_dense.num_classes)\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = Net_Diff().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_dense = dataset_dense.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_dense)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_dense[:num_train]\n",
    "    val_dataset = dataset_dense[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_dense[num_train + num_val:]\n",
    "    train_loader = DenseDataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    valid_loader = DenseDataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "    test_loader = DenseDataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "    model = Net_Diff().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"./data\"\n",
    "dataset_dense = TUDataset(\n",
    "    data_path,\n",
    "    name=\"IMDB-MULTI\",\n",
    "    transform=T.Compose([T.OneHotDegree(88), T.ToDense(max_nodes)]),\n",
    "    use_node_attr=True,\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes,\n",
    ")\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        if lin:\n",
    "            self.lin = torch.nn.Linear(out_channels, out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "    def bn(self, i, x):\n",
    "        batch_size, num_nodes, num_channels = x.size()\n",
    "        x = x.view(-1, num_channels)\n",
    "        x = getattr(self, f'bn{i}')(x)\n",
    "        x = x.view(batch_size, num_nodes, num_channels)\n",
    "        return x\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x = self.bn(1, self.conv1(x, adj, mask).relu())\n",
    "        x = self.bn(2, self.conv2(x, adj, mask).relu())\n",
    "        x = self.bn(3, self.conv3(x, adj, mask).relu())\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "        return x\n",
    "class Net_Diff(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_nodes = 64\n",
    "        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = DenseGCNConv(dataset_dense.num_features, 64)\n",
    "        num_nodes = 64\n",
    "        self.gnn2_pool = GNN(64, 64, num_nodes)\n",
    "        self.gnn2_embed = DenseGCNConv(64, 64)\n",
    "        self.gnn3_embed = DenseGCNConv(64, 64)\n",
    "        self.lin1 = torch.nn.Linear(64, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, dataset_dense.num_classes)\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = Net_Diff().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_dense = dataset_dense.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_dense)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_dense[:num_train]\n",
    "    val_dataset = dataset_dense[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_dense[num_train + num_val:]\n",
    "    train_loader = DenseDataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    valid_loader = DenseDataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "    test_loader = DenseDataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "    model = Net_Diff().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/COLLAB.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 1.1237, Val Acc: 0.3213, Test Acc: 0.3320\n",
      "Seed: 42, Epoch: 002, Loss: 1.0931, Val Acc: 0.3213, Test Acc: 0.3320\n",
      "Seed: 42, Epoch: 003, Loss: 1.0342, Val Acc: 0.3213, Test Acc: 0.3320\n",
      "Seed: 42, Epoch: 004, Loss: 0.9391, Val Acc: 0.4973, Test Acc: 0.5080\n",
      "Seed: 42, Epoch: 005, Loss: 0.8423, Val Acc: 0.6080, Test Acc: 0.6320\n",
      "Seed: 42, Epoch: 006, Loss: 0.7420, Val Acc: 0.5560, Test Acc: 0.5653\n",
      "Seed: 42, Epoch: 007, Loss: 0.6483, Val Acc: 0.5653, Test Acc: 0.5787\n",
      "Seed: 42, Epoch: 008, Loss: 0.5859, Val Acc: 0.5893, Test Acc: 0.6027\n",
      "Seed: 42, Epoch: 009, Loss: 0.5546, Val Acc: 0.6293, Test Acc: 0.6467\n",
      "Seed: 42, Epoch: 010, Loss: 0.5284, Val Acc: 0.6453, Test Acc: 0.6547\n",
      "Seed: 42, Epoch: 011, Loss: 0.5110, Val Acc: 0.6733, Test Acc: 0.6760\n",
      "Seed: 42, Epoch: 012, Loss: 0.4958, Val Acc: 0.6827, Test Acc: 0.6907\n",
      "Seed: 42, Epoch: 013, Loss: 0.4807, Val Acc: 0.7173, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 014, Loss: 0.4667, Val Acc: 0.7253, Test Acc: 0.7107\n",
      "Seed: 42, Epoch: 015, Loss: 0.4524, Val Acc: 0.7200, Test Acc: 0.7053\n",
      "Seed: 42, Epoch: 016, Loss: 0.4361, Val Acc: 0.7373, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 017, Loss: 0.4268, Val Acc: 0.7427, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 018, Loss: 0.4156, Val Acc: 0.8013, Test Acc: 0.7627\n",
      "Seed: 42, Epoch: 019, Loss: 0.4014, Val Acc: 0.7787, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 020, Loss: 0.3947, Val Acc: 0.7800, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 021, Loss: 0.3809, Val Acc: 0.8000, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 022, Loss: 0.3785, Val Acc: 0.7933, Test Acc: 0.7573\n"
     ]
    }
   ],
   "source": [
    "max_nodes = 500\n",
    "data_path = \"./data\"\n",
    "dataset_dense = TUDataset(\n",
    "    data_path,\n",
    "    name=\"COLLAB\",\n",
    "    transform=T.Compose([T.OneHotDegree(491), T.ToDense(max_nodes)]),\n",
    "    use_node_attr=True,\n",
    "    pre_filter=lambda data: data.num_nodes <= max_nodes,\n",
    ")\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        if lin:\n",
    "            self.lin = torch.nn.Linear(out_channels, out_channels)\n",
    "        else:\n",
    "            self.lin = None\n",
    "    def bn(self, i, x):\n",
    "        batch_size, num_nodes, num_channels = x.size()\n",
    "        x = x.view(-1, num_channels)\n",
    "        x = getattr(self, f'bn{i}')(x)\n",
    "        x = x.view(batch_size, num_nodes, num_channels)\n",
    "        return x\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        x = self.bn(1, self.conv1(x, adj, mask).relu())\n",
    "        x = self.bn(2, self.conv2(x, adj, mask).relu())\n",
    "        x = self.bn(3, self.conv3(x, adj, mask).relu())\n",
    "        if self.lin is not None:\n",
    "            x = self.lin(x).relu()\n",
    "        return x\n",
    "class Net_Diff(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_nodes = 64\n",
    "        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n",
    "        self.gnn1_embed = DenseGCNConv(dataset_dense.num_features, 64)\n",
    "        num_nodes = 64\n",
    "        self.gnn2_pool = GNN(64, 64, num_nodes)\n",
    "        self.gnn2_embed = DenseGCNConv(64, 64)\n",
    "        self.gnn3_embed = DenseGCNConv(64, 64)\n",
    "        self.lin1 = torch.nn.Linear(64, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, dataset_dense.num_classes)\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        s = self.gnn1_pool(x, adj, mask)\n",
    "        x = self.gnn1_embed(x, adj, mask)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "model = Net_Diff().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        loss = F.nll_loss(output, data.y.view(-1))\n",
    "        loss.backward()\n",
    "        total_loss += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output, _, _ = model(data.x, data.adj, data.mask)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += int(pred.eq(data.y.view(-1)).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_dense = dataset_dense.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_dense)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_dense[:num_train]\n",
    "    val_dataset = dataset_dense[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_dense[num_train + num_val:]\n",
    "    train_loader = DenseDataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DenseDataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DenseDataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = Net_Diff().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./data/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 0 --run_times=5 --patience=150 --epochs=500 --pooling='Diff'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
