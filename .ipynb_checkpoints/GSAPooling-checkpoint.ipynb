{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/outdated/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Union, Optional, Callable\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from torch_geometric.utils import softmax\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GCNConv, GATConv, ChebConv, GraphConv\n",
    "def uniform(size, tensor):\n",
    "    if tensor is not None:\n",
    "        bound = 1.0 / math.sqrt(size)\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "def topk(x, ratio, batch, min_score=None, tol=1e-7):\n",
    "    if min_score is not None:\n",
    "        scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n",
    "    else:\n",
    "        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "        cum_num_nodes = torch.cat(\n",
    "            [num_nodes.new_zeros(1),\n",
    "             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "        dense_x = x.new_full((batch_size * max_num_nodes, ),\n",
    "                             torch.finfo(x.dtype).min)\n",
    "        dense_x[index] = x\n",
    "        dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "        _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "        perm = perm + cum_num_nodes.view(-1, 1)\n",
    "        perm = perm.view(-1)\n",
    "        if isinstance(ratio, int):\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n",
    "            k = torch.min(k, num_nodes)\n",
    "        else:\n",
    "            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "        mask = [\n",
    "            torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "            i * max_num_nodes for i in range(batch_size)\n",
    "        ]\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "        perm = perm[mask]\n",
    "    return perm\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "class GSAPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n",
    "                        min_score=None, multiplier=1,\n",
    "                        non_linearity=torch.tanh,\n",
    "                        cus_drop_ratio =0):\n",
    "        super(GSAPool,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = pooling_ratio\n",
    "        self.alpha = alpha\n",
    "        self.sbtl_layer = GCNConv(in_channels,1)\n",
    "        self.fbtl_layer = nn.Linear(in_channels, 1)\n",
    "        self.fusion = GCNConv(in_channels,in_channels)\n",
    "        self.min_score = min_score\n",
    "        self.multiplier = multiplier\n",
    "        self.fusion_flag = 0\n",
    "        self.non_linearity = non_linearity\n",
    "        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n",
    "    def conv_selection(self, conv, in_channels, conv_type=0):\n",
    "        if(conv_type == 0):\n",
    "            out_channels = 1\n",
    "        elif(conv_type == 1):\n",
    "            out_channels = in_channels\n",
    "        if(conv == \"GCNConv\"):\n",
    "            return GCNConv(in_channels,out_channels)\n",
    "        elif(conv == \"ChebConv\"):\n",
    "            return ChebConv(in_channels,out_channels,1)\n",
    "        elif(conv == \"GCNConv\"):\n",
    "            return GCNConv(in_channels,out_channels)\n",
    "        elif(conv == \"GATConv\"):\n",
    "            return GATConv(in_channels,out_channels, heads=1, concat=True)\n",
    "        elif(conv == \"GraphConv\"):\n",
    "            return GraphConv(in_channels,out_channels)\n",
    "        else:\n",
    "            raise ValueError\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "        score_s = self.sbtl_layer(x,edge_index).squeeze()\n",
    "        score_f = self.fbtl_layer(x).squeeze()\n",
    "        score = score_s*self.alpha + score_f*(1-self.alpha)\n",
    "        score = score.unsqueeze(-1) if score.dim()==0 else score\n",
    "        if self.min_score is None:\n",
    "            score = self.non_linearity(score)\n",
    "        else:\n",
    "            score = softmax(score, batch)\n",
    "        sc = self.dropout(score)\n",
    "        perm = topk(sc, self.ratio, batch)\n",
    "        if(self.fusion_flag == 1):\n",
    "            x = self.fusion(x, edge_index)\n",
    "        x_ae = x[perm]\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        x = self.multiplier * x if self.multiplier != 1 else x\n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_attr = filter_adj(\n",
    "            edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "        return x, edge_index, edge_attr, batch, perm, x_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = GSAPool(64, pooling_ratio=0.7, alpha = 0.6, cus_drop_ratio = 0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = GSAPool(64, pooling_ratio=0.7, alpha = 0.6, cus_drop_ratio = 0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool1(x, edge_index, None, batch)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool2(x, edge_index, None, batch)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_nodes = 150\n",
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"MUTAG\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/DD.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time: 35.99 seconds\n",
      "Var Time: 1.78 seconds\n",
      "Average Memory: 1769.33 MB\n",
      "Average Best Val Acc: 0.8134\n",
      "Std Best Test Acc: 0.0129\n",
      "Average Test Acc: 0.7883\n"
     ]
    }
   ],
   "source": [
    "max_nodes = 500\n",
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"DD\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PROTEINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 157 for seed 45\n",
      "Early stopping at epoch 189 for seed 48\n",
      "Average Time: 23.17 seconds\n",
      "Var Time: 3.73 seconds\n",
      "Average Memory: 184.67 MB\n",
      "Average Best Val Acc: 0.7349\n",
      "Std Best Test Acc: 0.0309\n",
      "Average Test Acc: 0.6726\n"
     ]
    }
   ],
   "source": [
    "max_nodes = 700\n",
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"PROTEINS\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [45,46,47,48,49,50]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### NCI1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/NCI1.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time: 85.28 seconds\n",
      "Var Time: 0.10 seconds\n",
      "Average Memory: 154.00 MB\n",
      "Average Best Val Acc: 0.6894\n",
      "Std Best Test Acc: 0.0153\n",
      "Average Test Acc: 0.6775\n"
     ]
    }
   ],
   "source": [
    "max_nodes = 150\n",
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"NCI1\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [45,46,47]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCI109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 5\n"
     ]
    }
   ],
   "source": [
    "max_nodes = 1000\n",
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"NCI109\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [45,46,47]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-BINARY\", transform=T.Compose([T.OneHotDegree(136)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = GSAPool(64, pooling_ratio=0.9, alpha = 0.7, cus_drop_ratio = 0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = GSAPool(64, pooling_ratio=0.9, alpha = 0.7, cus_drop_ratio = 0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool1(x, edge_index, None, batch)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool2(x, edge_index, None, batch)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-MULTI\", transform=T.Compose([T.OneHotDegree(88)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = GSAPool(64, pooling_ratio=0.7, alpha = 0.7, cus_drop_ratio = 0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = GSAPool(64, pooling_ratio=0.7, alpha = 0.7, cus_drop_ratio = 0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool1(x, edge_index, None, batch)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool2(x, edge_index, None, batch)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/COLLAB.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 1.0887, Val Acc: 0.3213, Test Acc: 0.3320\n",
      "Seed: 42, Epoch: 002, Loss: 1.0804, Val Acc: 0.3213, Test Acc: 0.3320\n",
      "Seed: 42, Epoch: 003, Loss: 1.0723, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 004, Loss: 1.0645, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 005, Loss: 1.0565, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 006, Loss: 1.0478, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 007, Loss: 1.0357, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 008, Loss: 1.0158, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 009, Loss: 0.9826, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 010, Loss: 0.9420, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 011, Loss: 0.9162, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 012, Loss: 0.8927, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 013, Loss: 0.8763, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 014, Loss: 0.8607, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 015, Loss: 0.8496, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 016, Loss: 0.8398, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 017, Loss: 0.8299, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 018, Loss: 0.8229, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 019, Loss: 0.8137, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 020, Loss: 0.8036, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 021, Loss: 0.7940, Val Acc: 0.5067, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 022, Loss: 0.7818, Val Acc: 0.5280, Test Acc: 0.5293\n",
      "Seed: 42, Epoch: 023, Loss: 0.7682, Val Acc: 0.5667, Test Acc: 0.5453\n",
      "Seed: 42, Epoch: 024, Loss: 0.7474, Val Acc: 0.5773, Test Acc: 0.5400\n",
      "Seed: 42, Epoch: 025, Loss: 0.7274, Val Acc: 0.5760, Test Acc: 0.5347\n",
      "Seed: 42, Epoch: 026, Loss: 0.7161, Val Acc: 0.5760, Test Acc: 0.5360\n",
      "Seed: 42, Epoch: 027, Loss: 0.7063, Val Acc: 0.5907, Test Acc: 0.5467\n",
      "Seed: 42, Epoch: 028, Loss: 0.6966, Val Acc: 0.6173, Test Acc: 0.5733\n",
      "Seed: 42, Epoch: 029, Loss: 0.6878, Val Acc: 0.6720, Test Acc: 0.6347\n",
      "Seed: 42, Epoch: 030, Loss: 0.6785, Val Acc: 0.6733, Test Acc: 0.6320\n",
      "Seed: 42, Epoch: 031, Loss: 0.6681, Val Acc: 0.6787, Test Acc: 0.6400\n",
      "Seed: 42, Epoch: 032, Loss: 0.6575, Val Acc: 0.6933, Test Acc: 0.6533\n",
      "Seed: 42, Epoch: 033, Loss: 0.6472, Val Acc: 0.6987, Test Acc: 0.6613\n",
      "Seed: 42, Epoch: 034, Loss: 0.6353, Val Acc: 0.6827, Test Acc: 0.6733\n",
      "Seed: 42, Epoch: 035, Loss: 0.6230, Val Acc: 0.6733, Test Acc: 0.6693\n",
      "Seed: 42, Epoch: 036, Loss: 0.6092, Val Acc: 0.6800, Test Acc: 0.6693\n",
      "Seed: 42, Epoch: 037, Loss: 0.5973, Val Acc: 0.6653, Test Acc: 0.6520\n",
      "Seed: 42, Epoch: 038, Loss: 0.5889, Val Acc: 0.6840, Test Acc: 0.6693\n",
      "Seed: 42, Epoch: 039, Loss: 0.5774, Val Acc: 0.6747, Test Acc: 0.6760\n",
      "Seed: 42, Epoch: 040, Loss: 0.5705, Val Acc: 0.6760, Test Acc: 0.6760\n",
      "Seed: 42, Epoch: 041, Loss: 0.5716, Val Acc: 0.6547, Test Acc: 0.6747\n",
      "Seed: 42, Epoch: 042, Loss: 0.5664, Val Acc: 0.6747, Test Acc: 0.6787\n",
      "Seed: 42, Epoch: 043, Loss: 0.5580, Val Acc: 0.6907, Test Acc: 0.6920\n",
      "Seed: 42, Epoch: 044, Loss: 0.5520, Val Acc: 0.6840, Test Acc: 0.6907\n",
      "Seed: 42, Epoch: 045, Loss: 0.5467, Val Acc: 0.6760, Test Acc: 0.6920\n",
      "Seed: 42, Epoch: 046, Loss: 0.5404, Val Acc: 0.7013, Test Acc: 0.6907\n",
      "Seed: 42, Epoch: 047, Loss: 0.5402, Val Acc: 0.6867, Test Acc: 0.6973\n",
      "Seed: 42, Epoch: 048, Loss: 0.5333, Val Acc: 0.6907, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 049, Loss: 0.5294, Val Acc: 0.6893, Test Acc: 0.7040\n",
      "Seed: 42, Epoch: 050, Loss: 0.5262, Val Acc: 0.6973, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 051, Loss: 0.5262, Val Acc: 0.7013, Test Acc: 0.7040\n",
      "Seed: 42, Epoch: 052, Loss: 0.5213, Val Acc: 0.7080, Test Acc: 0.7040\n",
      "Seed: 42, Epoch: 053, Loss: 0.5167, Val Acc: 0.7027, Test Acc: 0.7013\n",
      "Seed: 42, Epoch: 054, Loss: 0.5156, Val Acc: 0.7080, Test Acc: 0.7120\n",
      "Seed: 42, Epoch: 055, Loss: 0.5123, Val Acc: 0.7080, Test Acc: 0.6987\n",
      "Seed: 42, Epoch: 056, Loss: 0.5092, Val Acc: 0.7187, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 057, Loss: 0.5068, Val Acc: 0.7293, Test Acc: 0.7080\n",
      "Seed: 42, Epoch: 058, Loss: 0.5038, Val Acc: 0.7267, Test Acc: 0.7053\n",
      "Seed: 42, Epoch: 059, Loss: 0.5048, Val Acc: 0.7293, Test Acc: 0.7147\n",
      "Seed: 42, Epoch: 060, Loss: 0.4996, Val Acc: 0.7120, Test Acc: 0.7093\n",
      "Seed: 42, Epoch: 061, Loss: 0.4974, Val Acc: 0.7267, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 062, Loss: 0.4974, Val Acc: 0.7093, Test Acc: 0.7093\n",
      "Seed: 42, Epoch: 063, Loss: 0.4954, Val Acc: 0.7347, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 064, Loss: 0.4905, Val Acc: 0.7120, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 065, Loss: 0.4875, Val Acc: 0.7227, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 066, Loss: 0.4884, Val Acc: 0.7173, Test Acc: 0.7053\n",
      "Seed: 42, Epoch: 067, Loss: 0.4862, Val Acc: 0.7307, Test Acc: 0.7093\n",
      "Seed: 42, Epoch: 068, Loss: 0.4873, Val Acc: 0.7160, Test Acc: 0.7147\n",
      "Seed: 42, Epoch: 069, Loss: 0.4793, Val Acc: 0.7400, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 070, Loss: 0.4784, Val Acc: 0.7173, Test Acc: 0.7187\n",
      "Seed: 42, Epoch: 071, Loss: 0.4773, Val Acc: 0.7387, Test Acc: 0.7187\n",
      "Seed: 42, Epoch: 072, Loss: 0.4750, Val Acc: 0.7267, Test Acc: 0.7107\n",
      "Seed: 42, Epoch: 073, Loss: 0.4760, Val Acc: 0.7427, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 074, Loss: 0.4735, Val Acc: 0.7453, Test Acc: 0.7227\n",
      "Seed: 42, Epoch: 075, Loss: 0.4722, Val Acc: 0.7493, Test Acc: 0.7240\n",
      "Seed: 42, Epoch: 076, Loss: 0.4708, Val Acc: 0.7493, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 077, Loss: 0.4694, Val Acc: 0.7547, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 078, Loss: 0.4671, Val Acc: 0.7547, Test Acc: 0.7227\n",
      "Seed: 42, Epoch: 079, Loss: 0.4663, Val Acc: 0.7533, Test Acc: 0.7240\n",
      "Seed: 42, Epoch: 080, Loss: 0.4647, Val Acc: 0.7573, Test Acc: 0.7280\n",
      "Seed: 42, Epoch: 081, Loss: 0.4630, Val Acc: 0.7600, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 082, Loss: 0.4629, Val Acc: 0.7600, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 083, Loss: 0.4603, Val Acc: 0.7573, Test Acc: 0.7280\n",
      "Seed: 42, Epoch: 084, Loss: 0.4604, Val Acc: 0.7613, Test Acc: 0.7307\n",
      "Seed: 42, Epoch: 085, Loss: 0.4592, Val Acc: 0.7653, Test Acc: 0.7280\n",
      "Seed: 42, Epoch: 086, Loss: 0.4584, Val Acc: 0.7640, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 087, Loss: 0.4560, Val Acc: 0.7640, Test Acc: 0.7280\n",
      "Seed: 42, Epoch: 088, Loss: 0.4565, Val Acc: 0.7600, Test Acc: 0.7173\n",
      "Seed: 42, Epoch: 089, Loss: 0.4554, Val Acc: 0.7640, Test Acc: 0.7293\n",
      "Seed: 42, Epoch: 090, Loss: 0.4524, Val Acc: 0.7600, Test Acc: 0.7240\n",
      "Seed: 42, Epoch: 091, Loss: 0.4522, Val Acc: 0.7600, Test Acc: 0.7307\n",
      "Seed: 42, Epoch: 092, Loss: 0.4517, Val Acc: 0.7640, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 093, Loss: 0.4505, Val Acc: 0.7653, Test Acc: 0.7307\n",
      "Seed: 42, Epoch: 094, Loss: 0.4487, Val Acc: 0.7640, Test Acc: 0.7280\n",
      "Seed: 42, Epoch: 095, Loss: 0.4483, Val Acc: 0.7640, Test Acc: 0.7280\n",
      "Seed: 42, Epoch: 096, Loss: 0.4470, Val Acc: 0.7613, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 097, Loss: 0.4459, Val Acc: 0.7640, Test Acc: 0.7213\n",
      "Seed: 42, Epoch: 098, Loss: 0.4466, Val Acc: 0.7587, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 099, Loss: 0.4441, Val Acc: 0.7587, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 100, Loss: 0.4465, Val Acc: 0.7613, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 101, Loss: 0.4441, Val Acc: 0.7600, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 102, Loss: 0.4426, Val Acc: 0.7600, Test Acc: 0.7307\n",
      "Seed: 42, Epoch: 103, Loss: 0.4404, Val Acc: 0.7600, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 104, Loss: 0.4380, Val Acc: 0.7640, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 105, Loss: 0.4414, Val Acc: 0.7680, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 106, Loss: 0.4394, Val Acc: 0.7613, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 107, Loss: 0.4384, Val Acc: 0.7573, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 108, Loss: 0.4380, Val Acc: 0.7613, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 109, Loss: 0.4381, Val Acc: 0.7653, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 110, Loss: 0.4351, Val Acc: 0.7627, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 111, Loss: 0.4357, Val Acc: 0.7587, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 112, Loss: 0.4346, Val Acc: 0.7613, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 113, Loss: 0.4333, Val Acc: 0.7613, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 114, Loss: 0.4334, Val Acc: 0.7653, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 115, Loss: 0.4334, Val Acc: 0.7613, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 116, Loss: 0.4332, Val Acc: 0.7667, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 117, Loss: 0.4318, Val Acc: 0.7627, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 118, Loss: 0.4292, Val Acc: 0.7627, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 119, Loss: 0.4293, Val Acc: 0.7533, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 120, Loss: 0.4310, Val Acc: 0.7600, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 121, Loss: 0.4328, Val Acc: 0.7693, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 122, Loss: 0.4303, Val Acc: 0.7533, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 123, Loss: 0.4285, Val Acc: 0.7600, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 124, Loss: 0.4276, Val Acc: 0.7587, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 125, Loss: 0.4259, Val Acc: 0.7547, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 126, Loss: 0.4236, Val Acc: 0.7627, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 127, Loss: 0.4245, Val Acc: 0.7640, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 128, Loss: 0.4230, Val Acc: 0.7613, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 129, Loss: 0.4237, Val Acc: 0.7627, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 130, Loss: 0.4236, Val Acc: 0.7587, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 131, Loss: 0.4225, Val Acc: 0.7587, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 132, Loss: 0.4207, Val Acc: 0.7613, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 133, Loss: 0.4229, Val Acc: 0.7600, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 134, Loss: 0.4189, Val Acc: 0.7627, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 135, Loss: 0.4190, Val Acc: 0.7600, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 136, Loss: 0.4159, Val Acc: 0.7627, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 137, Loss: 0.4160, Val Acc: 0.7653, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 138, Loss: 0.4152, Val Acc: 0.7640, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 139, Loss: 0.4172, Val Acc: 0.7613, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 140, Loss: 0.4152, Val Acc: 0.7680, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 141, Loss: 0.4124, Val Acc: 0.7627, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 142, Loss: 0.4112, Val Acc: 0.7667, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 143, Loss: 0.4125, Val Acc: 0.7627, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 144, Loss: 0.4130, Val Acc: 0.7653, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 145, Loss: 0.4114, Val Acc: 0.7680, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 146, Loss: 0.4094, Val Acc: 0.7653, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 147, Loss: 0.4087, Val Acc: 0.7680, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 148, Loss: 0.4104, Val Acc: 0.7640, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 149, Loss: 0.4086, Val Acc: 0.7680, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 150, Loss: 0.4064, Val Acc: 0.7613, Test Acc: 0.7547\n",
      "Seed: 42, Epoch: 151, Loss: 0.4057, Val Acc: 0.7720, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 152, Loss: 0.4061, Val Acc: 0.7653, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 153, Loss: 0.4051, Val Acc: 0.7613, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 154, Loss: 0.4038, Val Acc: 0.7667, Test Acc: 0.7560\n",
      "Seed: 42, Epoch: 155, Loss: 0.4036, Val Acc: 0.7640, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 156, Loss: 0.4018, Val Acc: 0.7653, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 157, Loss: 0.4027, Val Acc: 0.7627, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 158, Loss: 0.4033, Val Acc: 0.7680, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 159, Loss: 0.4072, Val Acc: 0.7493, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 160, Loss: 0.4032, Val Acc: 0.7573, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 161, Loss: 0.3985, Val Acc: 0.7640, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 162, Loss: 0.3991, Val Acc: 0.7640, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 163, Loss: 0.3984, Val Acc: 0.7613, Test Acc: 0.7587\n",
      "Seed: 42, Epoch: 164, Loss: 0.3970, Val Acc: 0.7627, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 165, Loss: 0.3955, Val Acc: 0.7653, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 166, Loss: 0.3934, Val Acc: 0.7627, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 167, Loss: 0.3947, Val Acc: 0.7680, Test Acc: 0.7520\n",
      "Seed: 42, Epoch: 168, Loss: 0.3947, Val Acc: 0.7600, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 169, Loss: 0.3928, Val Acc: 0.7760, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 170, Loss: 0.3919, Val Acc: 0.7680, Test Acc: 0.7520\n",
      "Seed: 42, Epoch: 171, Loss: 0.3943, Val Acc: 0.7680, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 172, Loss: 0.3909, Val Acc: 0.7613, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 173, Loss: 0.3903, Val Acc: 0.7627, Test Acc: 0.7587\n",
      "Seed: 42, Epoch: 174, Loss: 0.3862, Val Acc: 0.7600, Test Acc: 0.7587\n",
      "Seed: 42, Epoch: 175, Loss: 0.3877, Val Acc: 0.7667, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 176, Loss: 0.3865, Val Acc: 0.7667, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 177, Loss: 0.3861, Val Acc: 0.7587, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 178, Loss: 0.3874, Val Acc: 0.7627, Test Acc: 0.7573\n",
      "Seed: 42, Epoch: 179, Loss: 0.3852, Val Acc: 0.7640, Test Acc: 0.7560\n",
      "Seed: 42, Epoch: 180, Loss: 0.3835, Val Acc: 0.7653, Test Acc: 0.7720\n",
      "Seed: 42, Epoch: 181, Loss: 0.3818, Val Acc: 0.7653, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 182, Loss: 0.3837, Val Acc: 0.7573, Test Acc: 0.7547\n",
      "Seed: 42, Epoch: 183, Loss: 0.3838, Val Acc: 0.7667, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 184, Loss: 0.3814, Val Acc: 0.7613, Test Acc: 0.7587\n",
      "Seed: 42, Epoch: 185, Loss: 0.3800, Val Acc: 0.7707, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 186, Loss: 0.3820, Val Acc: 0.7613, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 187, Loss: 0.3790, Val Acc: 0.7693, Test Acc: 0.7587\n",
      "Seed: 42, Epoch: 188, Loss: 0.3783, Val Acc: 0.7573, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 189, Loss: 0.3771, Val Acc: 0.7693, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 190, Loss: 0.3764, Val Acc: 0.7600, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 191, Loss: 0.3747, Val Acc: 0.7627, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 192, Loss: 0.3749, Val Acc: 0.7640, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 193, Loss: 0.3761, Val Acc: 0.7733, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 194, Loss: 0.3737, Val Acc: 0.7667, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 195, Loss: 0.3742, Val Acc: 0.7653, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 196, Loss: 0.3709, Val Acc: 0.7653, Test Acc: 0.7547\n",
      "Seed: 42, Epoch: 197, Loss: 0.3709, Val Acc: 0.7720, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 198, Loss: 0.3757, Val Acc: 0.7533, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 199, Loss: 0.3724, Val Acc: 0.7733, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 200, Loss: 0.3743, Val Acc: 0.7587, Test Acc: 0.7480\n",
      "Seed: 43, Epoch: 001, Loss: 1.0865, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 002, Loss: 1.0798, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 003, Loss: 1.0733, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 004, Loss: 1.0670, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 005, Loss: 1.0604, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 006, Loss: 1.0523, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 007, Loss: 1.0388, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 008, Loss: 1.0075, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 009, Loss: 0.9648, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 010, Loss: 0.9350, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 011, Loss: 0.9105, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 012, Loss: 0.8898, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 013, Loss: 0.8698, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 014, Loss: 0.8548, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 015, Loss: 0.8418, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 016, Loss: 0.8302, Val Acc: 0.5067, Test Acc: 0.5173\n",
      "Seed: 43, Epoch: 017, Loss: 0.8231, Val Acc: 0.5600, Test Acc: 0.5547\n",
      "Seed: 43, Epoch: 018, Loss: 0.8147, Val Acc: 0.6187, Test Acc: 0.6147\n",
      "Seed: 43, Epoch: 019, Loss: 0.8099, Val Acc: 0.6120, Test Acc: 0.5827\n",
      "Seed: 43, Epoch: 020, Loss: 0.8029, Val Acc: 0.6067, Test Acc: 0.5773\n",
      "Seed: 43, Epoch: 021, Loss: 0.7955, Val Acc: 0.6107, Test Acc: 0.5813\n",
      "Seed: 43, Epoch: 022, Loss: 0.7864, Val Acc: 0.6200, Test Acc: 0.5920\n",
      "Seed: 43, Epoch: 023, Loss: 0.7771, Val Acc: 0.6133, Test Acc: 0.5933\n",
      "Seed: 43, Epoch: 024, Loss: 0.7655, Val Acc: 0.6400, Test Acc: 0.6240\n",
      "Seed: 43, Epoch: 025, Loss: 0.7504, Val Acc: 0.6467, Test Acc: 0.6293\n",
      "Seed: 43, Epoch: 026, Loss: 0.7279, Val Acc: 0.6533, Test Acc: 0.6373\n",
      "Seed: 43, Epoch: 027, Loss: 0.6994, Val Acc: 0.6560, Test Acc: 0.6400\n",
      "Seed: 43, Epoch: 028, Loss: 0.6739, Val Acc: 0.6613, Test Acc: 0.6400\n",
      "Seed: 43, Epoch: 029, Loss: 0.6517, Val Acc: 0.6667, Test Acc: 0.6427\n",
      "Seed: 43, Epoch: 030, Loss: 0.6347, Val Acc: 0.6733, Test Acc: 0.6453\n",
      "Seed: 43, Epoch: 031, Loss: 0.6210, Val Acc: 0.6973, Test Acc: 0.6747\n",
      "Seed: 43, Epoch: 032, Loss: 0.6094, Val Acc: 0.7467, Test Acc: 0.7187\n",
      "Seed: 43, Epoch: 033, Loss: 0.5965, Val Acc: 0.7560, Test Acc: 0.7427\n",
      "Seed: 43, Epoch: 034, Loss: 0.5888, Val Acc: 0.7493, Test Acc: 0.7360\n",
      "Seed: 43, Epoch: 035, Loss: 0.5816, Val Acc: 0.7667, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 036, Loss: 0.5700, Val Acc: 0.7693, Test Acc: 0.7427\n",
      "Seed: 43, Epoch: 037, Loss: 0.5638, Val Acc: 0.7667, Test Acc: 0.7373\n",
      "Seed: 43, Epoch: 038, Loss: 0.5563, Val Acc: 0.7693, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 039, Loss: 0.5519, Val Acc: 0.7693, Test Acc: 0.7427\n",
      "Seed: 43, Epoch: 040, Loss: 0.5484, Val Acc: 0.7680, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 041, Loss: 0.5443, Val Acc: 0.7693, Test Acc: 0.7413\n",
      "Seed: 43, Epoch: 042, Loss: 0.5386, Val Acc: 0.7693, Test Acc: 0.7440\n",
      "Seed: 43, Epoch: 043, Loss: 0.5355, Val Acc: 0.7720, Test Acc: 0.7440\n",
      "Seed: 43, Epoch: 044, Loss: 0.5307, Val Acc: 0.7720, Test Acc: 0.7427\n",
      "Seed: 43, Epoch: 045, Loss: 0.5254, Val Acc: 0.7653, Test Acc: 0.7493\n",
      "Seed: 43, Epoch: 046, Loss: 0.5257, Val Acc: 0.7667, Test Acc: 0.7453\n",
      "Seed: 43, Epoch: 047, Loss: 0.5195, Val Acc: 0.7693, Test Acc: 0.7480\n",
      "Seed: 43, Epoch: 048, Loss: 0.5136, Val Acc: 0.7653, Test Acc: 0.7520\n",
      "Seed: 43, Epoch: 049, Loss: 0.5122, Val Acc: 0.7600, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 050, Loss: 0.5123, Val Acc: 0.7720, Test Acc: 0.7453\n",
      "Seed: 43, Epoch: 051, Loss: 0.5080, Val Acc: 0.7667, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 052, Loss: 0.5029, Val Acc: 0.7653, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 053, Loss: 0.4996, Val Acc: 0.7707, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 054, Loss: 0.4962, Val Acc: 0.7733, Test Acc: 0.7520\n",
      "Seed: 43, Epoch: 055, Loss: 0.4948, Val Acc: 0.7747, Test Acc: 0.7560\n",
      "Seed: 43, Epoch: 056, Loss: 0.4906, Val Acc: 0.7720, Test Acc: 0.7453\n",
      "Seed: 43, Epoch: 057, Loss: 0.4885, Val Acc: 0.7773, Test Acc: 0.7480\n",
      "Seed: 43, Epoch: 058, Loss: 0.4864, Val Acc: 0.7773, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 059, Loss: 0.4852, Val Acc: 0.7747, Test Acc: 0.7520\n",
      "Seed: 43, Epoch: 060, Loss: 0.4846, Val Acc: 0.7707, Test Acc: 0.7480\n",
      "Seed: 43, Epoch: 061, Loss: 0.4800, Val Acc: 0.7827, Test Acc: 0.7560\n",
      "Seed: 43, Epoch: 062, Loss: 0.4779, Val Acc: 0.7787, Test Acc: 0.7493\n",
      "Seed: 43, Epoch: 063, Loss: 0.4764, Val Acc: 0.7747, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 064, Loss: 0.4796, Val Acc: 0.7720, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 065, Loss: 0.4741, Val Acc: 0.7760, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 066, Loss: 0.4723, Val Acc: 0.7760, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 067, Loss: 0.4732, Val Acc: 0.7827, Test Acc: 0.7613\n",
      "Seed: 43, Epoch: 068, Loss: 0.4696, Val Acc: 0.7827, Test Acc: 0.7560\n",
      "Seed: 43, Epoch: 069, Loss: 0.4666, Val Acc: 0.7853, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 070, Loss: 0.4637, Val Acc: 0.7853, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 071, Loss: 0.4632, Val Acc: 0.7827, Test Acc: 0.7560\n",
      "Seed: 43, Epoch: 072, Loss: 0.4605, Val Acc: 0.7827, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 073, Loss: 0.4609, Val Acc: 0.7880, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 074, Loss: 0.4572, Val Acc: 0.7827, Test Acc: 0.7613\n",
      "Seed: 43, Epoch: 075, Loss: 0.4549, Val Acc: 0.7787, Test Acc: 0.7573\n",
      "Seed: 43, Epoch: 076, Loss: 0.4545, Val Acc: 0.7840, Test Acc: 0.7747\n",
      "Seed: 43, Epoch: 077, Loss: 0.4508, Val Acc: 0.7907, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 078, Loss: 0.4494, Val Acc: 0.7733, Test Acc: 0.7627\n",
      "Seed: 43, Epoch: 079, Loss: 0.4497, Val Acc: 0.7853, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 080, Loss: 0.4444, Val Acc: 0.7867, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 081, Loss: 0.4463, Val Acc: 0.7907, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 082, Loss: 0.4455, Val Acc: 0.7760, Test Acc: 0.7640\n",
      "Seed: 43, Epoch: 083, Loss: 0.4447, Val Acc: 0.7920, Test Acc: 0.7707\n",
      "Seed: 43, Epoch: 084, Loss: 0.4387, Val Acc: 0.7853, Test Acc: 0.7707\n",
      "Seed: 43, Epoch: 085, Loss: 0.4367, Val Acc: 0.7787, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 086, Loss: 0.4346, Val Acc: 0.7853, Test Acc: 0.7707\n",
      "Seed: 43, Epoch: 087, Loss: 0.4333, Val Acc: 0.7867, Test Acc: 0.7707\n",
      "Seed: 43, Epoch: 088, Loss: 0.4301, Val Acc: 0.7947, Test Acc: 0.7707\n",
      "Seed: 43, Epoch: 089, Loss: 0.4297, Val Acc: 0.7867, Test Acc: 0.7707\n",
      "Seed: 43, Epoch: 090, Loss: 0.4278, Val Acc: 0.7880, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 091, Loss: 0.4263, Val Acc: 0.7853, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 092, Loss: 0.4241, Val Acc: 0.7920, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 093, Loss: 0.4210, Val Acc: 0.7867, Test Acc: 0.7707\n",
      "Seed: 43, Epoch: 094, Loss: 0.4203, Val Acc: 0.7880, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 095, Loss: 0.4175, Val Acc: 0.7933, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 096, Loss: 0.4169, Val Acc: 0.7933, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 097, Loss: 0.4165, Val Acc: 0.7933, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 098, Loss: 0.4153, Val Acc: 0.7907, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 099, Loss: 0.4145, Val Acc: 0.7867, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 100, Loss: 0.4111, Val Acc: 0.8013, Test Acc: 0.7747\n",
      "Seed: 43, Epoch: 101, Loss: 0.4070, Val Acc: 0.7920, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 102, Loss: 0.4036, Val Acc: 0.7920, Test Acc: 0.7747\n",
      "Seed: 43, Epoch: 103, Loss: 0.4006, Val Acc: 0.7907, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 104, Loss: 0.4003, Val Acc: 0.7933, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 105, Loss: 0.3984, Val Acc: 0.7960, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 106, Loss: 0.3995, Val Acc: 0.7920, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 107, Loss: 0.3956, Val Acc: 0.8000, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 108, Loss: 0.3915, Val Acc: 0.7947, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 109, Loss: 0.3886, Val Acc: 0.7853, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 110, Loss: 0.3865, Val Acc: 0.7947, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 111, Loss: 0.3892, Val Acc: 0.7920, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 112, Loss: 0.3880, Val Acc: 0.7893, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 113, Loss: 0.3864, Val Acc: 0.7880, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 114, Loss: 0.3810, Val Acc: 0.7840, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 115, Loss: 0.3826, Val Acc: 0.7960, Test Acc: 0.7747\n",
      "Seed: 43, Epoch: 116, Loss: 0.3814, Val Acc: 0.7920, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 117, Loss: 0.3800, Val Acc: 0.7960, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 118, Loss: 0.3771, Val Acc: 0.7840, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 119, Loss: 0.3788, Val Acc: 0.7933, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 120, Loss: 0.3729, Val Acc: 0.7813, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 121, Loss: 0.3740, Val Acc: 0.7960, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 122, Loss: 0.3708, Val Acc: 0.7933, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 123, Loss: 0.3686, Val Acc: 0.7840, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 124, Loss: 0.3666, Val Acc: 0.7880, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 125, Loss: 0.3719, Val Acc: 0.7840, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 126, Loss: 0.3688, Val Acc: 0.7920, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 127, Loss: 0.3626, Val Acc: 0.7960, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 128, Loss: 0.3603, Val Acc: 0.7867, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 129, Loss: 0.3558, Val Acc: 0.7880, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 130, Loss: 0.3577, Val Acc: 0.7893, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 131, Loss: 0.3568, Val Acc: 0.7973, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 132, Loss: 0.3545, Val Acc: 0.7920, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 133, Loss: 0.3585, Val Acc: 0.8000, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 134, Loss: 0.3535, Val Acc: 0.7920, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 135, Loss: 0.3485, Val Acc: 0.7947, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 136, Loss: 0.3502, Val Acc: 0.7987, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 137, Loss: 0.3476, Val Acc: 0.7893, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 138, Loss: 0.3465, Val Acc: 0.8040, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 139, Loss: 0.3453, Val Acc: 0.7893, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 140, Loss: 0.3491, Val Acc: 0.8053, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 141, Loss: 0.3439, Val Acc: 0.8120, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 142, Loss: 0.3407, Val Acc: 0.7947, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 143, Loss: 0.3380, Val Acc: 0.8027, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 144, Loss: 0.3375, Val Acc: 0.7933, Test Acc: 0.7853\n",
      "Seed: 43, Epoch: 145, Loss: 0.3344, Val Acc: 0.7960, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 146, Loss: 0.3357, Val Acc: 0.8027, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 147, Loss: 0.3337, Val Acc: 0.8013, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 148, Loss: 0.3333, Val Acc: 0.7920, Test Acc: 0.7920\n",
      "Seed: 43, Epoch: 149, Loss: 0.3333, Val Acc: 0.8053, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 150, Loss: 0.3332, Val Acc: 0.8053, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 151, Loss: 0.3302, Val Acc: 0.7933, Test Acc: 0.7933\n",
      "Seed: 43, Epoch: 152, Loss: 0.3296, Val Acc: 0.8147, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 153, Loss: 0.3341, Val Acc: 0.8067, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 154, Loss: 0.3325, Val Acc: 0.7893, Test Acc: 0.7960\n",
      "Seed: 43, Epoch: 155, Loss: 0.3262, Val Acc: 0.8013, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 156, Loss: 0.3266, Val Acc: 0.8040, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 157, Loss: 0.3237, Val Acc: 0.7947, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 158, Loss: 0.3231, Val Acc: 0.7987, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 159, Loss: 0.3231, Val Acc: 0.8067, Test Acc: 0.7707\n",
      "Seed: 43, Epoch: 160, Loss: 0.3248, Val Acc: 0.8040, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 161, Loss: 0.3185, Val Acc: 0.8107, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 162, Loss: 0.3155, Val Acc: 0.8000, Test Acc: 0.7933\n",
      "Seed: 43, Epoch: 163, Loss: 0.3151, Val Acc: 0.8053, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 164, Loss: 0.3145, Val Acc: 0.8040, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 165, Loss: 0.3224, Val Acc: 0.8027, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 166, Loss: 0.3166, Val Acc: 0.7960, Test Acc: 0.7987\n",
      "Seed: 43, Epoch: 167, Loss: 0.3147, Val Acc: 0.8107, Test Acc: 0.7853\n",
      "Seed: 43, Epoch: 168, Loss: 0.3133, Val Acc: 0.8067, Test Acc: 0.7947\n",
      "Seed: 43, Epoch: 169, Loss: 0.3145, Val Acc: 0.8067, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 170, Loss: 0.3107, Val Acc: 0.8067, Test Acc: 0.7853\n",
      "Seed: 43, Epoch: 171, Loss: 0.3089, Val Acc: 0.8040, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 172, Loss: 0.3094, Val Acc: 0.7947, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 173, Loss: 0.3148, Val Acc: 0.8053, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 174, Loss: 0.3063, Val Acc: 0.8013, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 175, Loss: 0.3077, Val Acc: 0.8013, Test Acc: 0.7933\n",
      "Seed: 43, Epoch: 176, Loss: 0.3109, Val Acc: 0.7987, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 177, Loss: 0.3133, Val Acc: 0.8053, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 178, Loss: 0.3109, Val Acc: 0.7987, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 179, Loss: 0.3022, Val Acc: 0.8067, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 180, Loss: 0.3028, Val Acc: 0.7973, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 181, Loss: 0.3016, Val Acc: 0.7947, Test Acc: 0.7987\n",
      "Seed: 43, Epoch: 182, Loss: 0.3013, Val Acc: 0.8080, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 183, Loss: 0.2982, Val Acc: 0.8107, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 184, Loss: 0.2983, Val Acc: 0.8013, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 185, Loss: 0.2975, Val Acc: 0.8000, Test Acc: 0.8013\n",
      "Seed: 43, Epoch: 186, Loss: 0.2968, Val Acc: 0.8093, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 187, Loss: 0.2990, Val Acc: 0.8080, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 188, Loss: 0.3008, Val Acc: 0.8040, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 189, Loss: 0.3035, Val Acc: 0.8067, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 190, Loss: 0.3004, Val Acc: 0.8040, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 191, Loss: 0.2977, Val Acc: 0.8093, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 192, Loss: 0.3007, Val Acc: 0.8040, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 193, Loss: 0.3013, Val Acc: 0.8053, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 194, Loss: 0.2943, Val Acc: 0.7987, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 195, Loss: 0.2951, Val Acc: 0.8027, Test Acc: 0.8000\n",
      "Seed: 43, Epoch: 196, Loss: 0.2920, Val Acc: 0.8067, Test Acc: 0.7920\n",
      "Seed: 43, Epoch: 197, Loss: 0.2885, Val Acc: 0.8120, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 198, Loss: 0.2912, Val Acc: 0.8107, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 199, Loss: 0.2890, Val Acc: 0.8080, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 200, Loss: 0.2993, Val Acc: 0.8053, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 001, Loss: 1.1059, Val Acc: 0.1493, Test Acc: 0.1600\n",
      "Seed: 44, Epoch: 002, Loss: 1.0956, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 003, Loss: 1.0850, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 004, Loss: 1.0745, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 005, Loss: 1.0638, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 006, Loss: 1.0507, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 007, Loss: 1.0315, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 008, Loss: 0.9959, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 009, Loss: 0.9508, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 010, Loss: 0.9172, Val Acc: 0.5760, Test Acc: 0.5933\n",
      "Seed: 44, Epoch: 011, Loss: 0.8885, Val Acc: 0.6147, Test Acc: 0.6080\n",
      "Seed: 44, Epoch: 012, Loss: 0.8573, Val Acc: 0.6400, Test Acc: 0.6280\n",
      "Seed: 44, Epoch: 013, Loss: 0.8286, Val Acc: 0.6307, Test Acc: 0.6227\n",
      "Seed: 44, Epoch: 014, Loss: 0.8028, Val Acc: 0.6747, Test Acc: 0.6333\n",
      "Seed: 44, Epoch: 015, Loss: 0.7774, Val Acc: 0.6720, Test Acc: 0.6400\n",
      "Seed: 44, Epoch: 016, Loss: 0.7565, Val Acc: 0.6907, Test Acc: 0.6640\n",
      "Seed: 44, Epoch: 017, Loss: 0.7368, Val Acc: 0.7040, Test Acc: 0.6813\n",
      "Seed: 44, Epoch: 018, Loss: 0.7174, Val Acc: 0.6973, Test Acc: 0.6880\n",
      "Seed: 44, Epoch: 019, Loss: 0.7032, Val Acc: 0.6907, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 020, Loss: 0.6883, Val Acc: 0.7080, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 021, Loss: 0.6769, Val Acc: 0.7107, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 022, Loss: 0.6661, Val Acc: 0.7000, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 023, Loss: 0.6562, Val Acc: 0.7093, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 024, Loss: 0.6479, Val Acc: 0.7053, Test Acc: 0.6907\n",
      "Seed: 44, Epoch: 025, Loss: 0.6392, Val Acc: 0.7120, Test Acc: 0.6987\n",
      "Seed: 44, Epoch: 026, Loss: 0.6300, Val Acc: 0.6987, Test Acc: 0.6880\n",
      "Seed: 44, Epoch: 027, Loss: 0.6246, Val Acc: 0.7053, Test Acc: 0.6947\n",
      "Seed: 44, Epoch: 028, Loss: 0.6166, Val Acc: 0.7053, Test Acc: 0.6960\n",
      "Seed: 44, Epoch: 029, Loss: 0.6129, Val Acc: 0.7013, Test Acc: 0.6960\n",
      "Seed: 44, Epoch: 030, Loss: 0.6081, Val Acc: 0.7053, Test Acc: 0.6987\n",
      "Seed: 44, Epoch: 031, Loss: 0.6006, Val Acc: 0.7013, Test Acc: 0.6893\n",
      "Seed: 44, Epoch: 032, Loss: 0.5948, Val Acc: 0.7000, Test Acc: 0.6947\n",
      "Seed: 44, Epoch: 033, Loss: 0.5903, Val Acc: 0.7040, Test Acc: 0.6947\n",
      "Seed: 44, Epoch: 034, Loss: 0.5845, Val Acc: 0.7000, Test Acc: 0.6880\n",
      "Seed: 44, Epoch: 035, Loss: 0.5797, Val Acc: 0.7053, Test Acc: 0.6947\n",
      "Seed: 44, Epoch: 036, Loss: 0.5800, Val Acc: 0.7040, Test Acc: 0.6960\n",
      "Seed: 44, Epoch: 037, Loss: 0.5705, Val Acc: 0.7053, Test Acc: 0.6973\n",
      "Seed: 44, Epoch: 038, Loss: 0.5692, Val Acc: 0.7080, Test Acc: 0.6987\n",
      "Seed: 44, Epoch: 039, Loss: 0.5650, Val Acc: 0.7120, Test Acc: 0.7053\n",
      "Seed: 44, Epoch: 040, Loss: 0.5628, Val Acc: 0.7253, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 041, Loss: 0.5599, Val Acc: 0.7253, Test Acc: 0.7320\n",
      "Seed: 44, Epoch: 042, Loss: 0.5592, Val Acc: 0.7267, Test Acc: 0.7253\n",
      "Seed: 44, Epoch: 043, Loss: 0.5584, Val Acc: 0.7240, Test Acc: 0.7280\n",
      "Seed: 44, Epoch: 044, Loss: 0.5540, Val Acc: 0.7307, Test Acc: 0.7307\n",
      "Seed: 44, Epoch: 045, Loss: 0.5540, Val Acc: 0.7253, Test Acc: 0.7187\n",
      "Seed: 44, Epoch: 046, Loss: 0.5513, Val Acc: 0.7307, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 047, Loss: 0.5457, Val Acc: 0.7280, Test Acc: 0.7253\n",
      "Seed: 44, Epoch: 048, Loss: 0.5434, Val Acc: 0.7267, Test Acc: 0.7320\n",
      "Seed: 44, Epoch: 049, Loss: 0.5448, Val Acc: 0.7333, Test Acc: 0.7253\n",
      "Seed: 44, Epoch: 050, Loss: 0.5415, Val Acc: 0.7360, Test Acc: 0.7213\n",
      "Seed: 44, Epoch: 051, Loss: 0.5394, Val Acc: 0.7333, Test Acc: 0.7373\n",
      "Seed: 44, Epoch: 052, Loss: 0.5390, Val Acc: 0.7333, Test Acc: 0.7347\n",
      "Seed: 44, Epoch: 053, Loss: 0.5398, Val Acc: 0.7347, Test Acc: 0.7347\n",
      "Seed: 44, Epoch: 054, Loss: 0.5371, Val Acc: 0.7320, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 055, Loss: 0.5346, Val Acc: 0.7333, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 056, Loss: 0.5345, Val Acc: 0.7387, Test Acc: 0.7240\n",
      "Seed: 44, Epoch: 057, Loss: 0.5327, Val Acc: 0.7333, Test Acc: 0.7413\n",
      "Seed: 44, Epoch: 058, Loss: 0.5319, Val Acc: 0.7387, Test Acc: 0.7347\n",
      "Seed: 44, Epoch: 059, Loss: 0.5311, Val Acc: 0.7373, Test Acc: 0.7360\n",
      "Seed: 44, Epoch: 060, Loss: 0.5325, Val Acc: 0.7347, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 061, Loss: 0.5303, Val Acc: 0.7347, Test Acc: 0.7347\n",
      "Seed: 44, Epoch: 062, Loss: 0.5290, Val Acc: 0.7333, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 063, Loss: 0.5262, Val Acc: 0.7373, Test Acc: 0.7360\n",
      "Seed: 44, Epoch: 064, Loss: 0.5312, Val Acc: 0.7293, Test Acc: 0.7293\n",
      "Seed: 44, Epoch: 065, Loss: 0.5277, Val Acc: 0.7333, Test Acc: 0.7373\n",
      "Seed: 44, Epoch: 066, Loss: 0.5227, Val Acc: 0.7400, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 067, Loss: 0.5232, Val Acc: 0.7347, Test Acc: 0.7373\n",
      "Seed: 44, Epoch: 068, Loss: 0.5232, Val Acc: 0.7387, Test Acc: 0.7373\n",
      "Seed: 44, Epoch: 069, Loss: 0.5180, Val Acc: 0.7320, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 070, Loss: 0.5182, Val Acc: 0.7360, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 071, Loss: 0.5153, Val Acc: 0.7360, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 072, Loss: 0.5158, Val Acc: 0.7387, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 073, Loss: 0.5137, Val Acc: 0.7400, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 074, Loss: 0.5120, Val Acc: 0.7413, Test Acc: 0.7347\n",
      "Seed: 44, Epoch: 075, Loss: 0.5129, Val Acc: 0.7413, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 076, Loss: 0.5086, Val Acc: 0.7427, Test Acc: 0.7373\n",
      "Seed: 44, Epoch: 077, Loss: 0.5098, Val Acc: 0.7453, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 078, Loss: 0.5086, Val Acc: 0.7440, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 079, Loss: 0.5050, Val Acc: 0.7440, Test Acc: 0.7413\n",
      "Seed: 44, Epoch: 080, Loss: 0.5046, Val Acc: 0.7467, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 081, Loss: 0.5040, Val Acc: 0.7400, Test Acc: 0.7293\n",
      "Seed: 44, Epoch: 082, Loss: 0.5013, Val Acc: 0.7520, Test Acc: 0.7413\n",
      "Seed: 44, Epoch: 083, Loss: 0.5011, Val Acc: 0.7413, Test Acc: 0.7360\n",
      "Seed: 44, Epoch: 084, Loss: 0.4980, Val Acc: 0.7547, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 085, Loss: 0.4999, Val Acc: 0.7480, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 086, Loss: 0.4955, Val Acc: 0.7400, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 087, Loss: 0.4964, Val Acc: 0.7520, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 088, Loss: 0.4942, Val Acc: 0.7547, Test Acc: 0.7360\n",
      "Seed: 44, Epoch: 089, Loss: 0.4920, Val Acc: 0.7480, Test Acc: 0.7360\n",
      "Seed: 44, Epoch: 090, Loss: 0.4894, Val Acc: 0.7480, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 091, Loss: 0.4902, Val Acc: 0.7560, Test Acc: 0.7413\n",
      "Seed: 44, Epoch: 092, Loss: 0.4892, Val Acc: 0.7533, Test Acc: 0.7373\n",
      "Seed: 44, Epoch: 093, Loss: 0.4895, Val Acc: 0.7480, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 094, Loss: 0.4863, Val Acc: 0.7587, Test Acc: 0.7307\n",
      "Seed: 44, Epoch: 095, Loss: 0.4853, Val Acc: 0.7533, Test Acc: 0.7320\n",
      "Seed: 44, Epoch: 096, Loss: 0.4841, Val Acc: 0.7560, Test Acc: 0.7307\n",
      "Seed: 44, Epoch: 097, Loss: 0.4827, Val Acc: 0.7533, Test Acc: 0.7413\n",
      "Seed: 44, Epoch: 098, Loss: 0.4796, Val Acc: 0.7613, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 099, Loss: 0.4781, Val Acc: 0.7533, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 100, Loss: 0.4790, Val Acc: 0.7493, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 101, Loss: 0.4768, Val Acc: 0.7627, Test Acc: 0.7347\n",
      "Seed: 44, Epoch: 102, Loss: 0.4753, Val Acc: 0.7493, Test Acc: 0.7360\n",
      "Seed: 44, Epoch: 103, Loss: 0.4781, Val Acc: 0.7573, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 104, Loss: 0.4736, Val Acc: 0.7653, Test Acc: 0.7360\n",
      "Seed: 44, Epoch: 105, Loss: 0.4733, Val Acc: 0.7520, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 106, Loss: 0.4697, Val Acc: 0.7627, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 107, Loss: 0.4693, Val Acc: 0.7573, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 108, Loss: 0.4675, Val Acc: 0.7480, Test Acc: 0.7320\n",
      "Seed: 44, Epoch: 109, Loss: 0.4689, Val Acc: 0.7640, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 110, Loss: 0.4618, Val Acc: 0.7627, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 111, Loss: 0.4599, Val Acc: 0.7600, Test Acc: 0.7373\n",
      "Seed: 44, Epoch: 112, Loss: 0.4584, Val Acc: 0.7493, Test Acc: 0.7360\n",
      "Seed: 44, Epoch: 113, Loss: 0.4612, Val Acc: 0.7667, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 114, Loss: 0.4572, Val Acc: 0.7613, Test Acc: 0.7493\n",
      "Seed: 44, Epoch: 115, Loss: 0.4515, Val Acc: 0.7600, Test Acc: 0.7520\n",
      "Seed: 44, Epoch: 116, Loss: 0.4501, Val Acc: 0.7573, Test Acc: 0.7520\n",
      "Seed: 44, Epoch: 117, Loss: 0.4495, Val Acc: 0.7680, Test Acc: 0.7480\n",
      "Seed: 44, Epoch: 118, Loss: 0.4469, Val Acc: 0.7560, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 119, Loss: 0.4518, Val Acc: 0.7627, Test Acc: 0.7520\n",
      "Seed: 44, Epoch: 120, Loss: 0.4477, Val Acc: 0.7520, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 121, Loss: 0.4462, Val Acc: 0.7547, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 122, Loss: 0.4449, Val Acc: 0.7680, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 123, Loss: 0.4422, Val Acc: 0.7640, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 124, Loss: 0.4454, Val Acc: 0.7613, Test Acc: 0.7507\n",
      "Seed: 44, Epoch: 125, Loss: 0.4438, Val Acc: 0.7520, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 126, Loss: 0.4379, Val Acc: 0.7547, Test Acc: 0.7480\n",
      "Seed: 44, Epoch: 127, Loss: 0.4364, Val Acc: 0.7507, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 128, Loss: 0.4362, Val Acc: 0.7560, Test Acc: 0.7480\n",
      "Seed: 44, Epoch: 129, Loss: 0.4357, Val Acc: 0.7547, Test Acc: 0.7507\n",
      "Seed: 44, Epoch: 130, Loss: 0.4359, Val Acc: 0.7413, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 131, Loss: 0.4335, Val Acc: 0.7533, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 132, Loss: 0.4313, Val Acc: 0.7640, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 133, Loss: 0.4281, Val Acc: 0.7560, Test Acc: 0.7480\n",
      "Seed: 44, Epoch: 134, Loss: 0.4316, Val Acc: 0.7573, Test Acc: 0.7480\n",
      "Seed: 44, Epoch: 135, Loss: 0.4260, Val Acc: 0.7587, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 136, Loss: 0.4296, Val Acc: 0.7480, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 137, Loss: 0.4340, Val Acc: 0.7467, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 138, Loss: 0.4409, Val Acc: 0.7600, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 139, Loss: 0.4329, Val Acc: 0.7640, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 140, Loss: 0.4291, Val Acc: 0.7600, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 141, Loss: 0.4273, Val Acc: 0.7560, Test Acc: 0.7520\n",
      "Seed: 44, Epoch: 142, Loss: 0.4205, Val Acc: 0.7520, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 143, Loss: 0.4201, Val Acc: 0.7707, Test Acc: 0.7480\n",
      "Seed: 44, Epoch: 144, Loss: 0.4227, Val Acc: 0.7680, Test Acc: 0.7613\n",
      "Seed: 44, Epoch: 145, Loss: 0.4239, Val Acc: 0.7627, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 146, Loss: 0.4214, Val Acc: 0.7587, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 147, Loss: 0.4227, Val Acc: 0.7587, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 148, Loss: 0.4225, Val Acc: 0.7707, Test Acc: 0.7640\n",
      "Seed: 44, Epoch: 149, Loss: 0.4187, Val Acc: 0.7653, Test Acc: 0.7653\n",
      "Seed: 44, Epoch: 150, Loss: 0.4186, Val Acc: 0.7853, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 151, Loss: 0.4179, Val Acc: 0.7760, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 152, Loss: 0.4179, Val Acc: 0.7680, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 153, Loss: 0.4149, Val Acc: 0.7640, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 154, Loss: 0.4155, Val Acc: 0.7667, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 155, Loss: 0.4113, Val Acc: 0.7813, Test Acc: 0.7507\n",
      "Seed: 44, Epoch: 156, Loss: 0.4128, Val Acc: 0.7760, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 157, Loss: 0.4097, Val Acc: 0.7653, Test Acc: 0.7680\n",
      "Seed: 44, Epoch: 158, Loss: 0.4126, Val Acc: 0.7707, Test Acc: 0.7653\n",
      "Seed: 44, Epoch: 159, Loss: 0.4109, Val Acc: 0.7733, Test Acc: 0.7640\n",
      "Seed: 44, Epoch: 160, Loss: 0.4055, Val Acc: 0.7627, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 161, Loss: 0.4056, Val Acc: 0.7587, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 162, Loss: 0.4077, Val Acc: 0.7720, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 163, Loss: 0.4058, Val Acc: 0.7680, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 164, Loss: 0.4037, Val Acc: 0.7693, Test Acc: 0.7613\n",
      "Seed: 44, Epoch: 165, Loss: 0.4021, Val Acc: 0.7707, Test Acc: 0.7640\n",
      "Seed: 44, Epoch: 166, Loss: 0.4046, Val Acc: 0.7800, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 167, Loss: 0.4031, Val Acc: 0.7613, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 168, Loss: 0.3992, Val Acc: 0.7667, Test Acc: 0.7613\n",
      "Seed: 44, Epoch: 169, Loss: 0.3995, Val Acc: 0.7613, Test Acc: 0.7507\n",
      "Seed: 44, Epoch: 170, Loss: 0.4122, Val Acc: 0.7587, Test Acc: 0.7560\n",
      "Seed: 44, Epoch: 171, Loss: 0.4000, Val Acc: 0.7720, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 172, Loss: 0.4000, Val Acc: 0.7667, Test Acc: 0.7560\n",
      "Seed: 44, Epoch: 173, Loss: 0.3971, Val Acc: 0.7680, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 174, Loss: 0.3987, Val Acc: 0.7613, Test Acc: 0.7680\n",
      "Seed: 44, Epoch: 175, Loss: 0.3978, Val Acc: 0.7707, Test Acc: 0.7653\n",
      "Seed: 44, Epoch: 176, Loss: 0.3909, Val Acc: 0.7707, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 177, Loss: 0.3919, Val Acc: 0.7653, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 178, Loss: 0.3873, Val Acc: 0.7680, Test Acc: 0.7560\n",
      "Seed: 44, Epoch: 179, Loss: 0.3894, Val Acc: 0.7653, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 180, Loss: 0.3891, Val Acc: 0.7747, Test Acc: 0.7613\n",
      "Seed: 44, Epoch: 181, Loss: 0.3859, Val Acc: 0.7680, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 182, Loss: 0.3890, Val Acc: 0.7667, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 183, Loss: 0.3884, Val Acc: 0.7693, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 184, Loss: 0.3891, Val Acc: 0.7720, Test Acc: 0.7413\n",
      "Seed: 44, Epoch: 185, Loss: 0.3869, Val Acc: 0.7680, Test Acc: 0.7520\n",
      "Seed: 44, Epoch: 186, Loss: 0.3893, Val Acc: 0.7760, Test Acc: 0.7613\n",
      "Seed: 44, Epoch: 187, Loss: 0.3861, Val Acc: 0.7627, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 188, Loss: 0.3854, Val Acc: 0.7667, Test Acc: 0.7640\n",
      "Seed: 44, Epoch: 189, Loss: 0.3844, Val Acc: 0.7720, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 190, Loss: 0.3850, Val Acc: 0.7747, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 191, Loss: 0.3856, Val Acc: 0.7507, Test Acc: 0.7507\n",
      "Seed: 44, Epoch: 192, Loss: 0.4106, Val Acc: 0.7613, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 193, Loss: 0.3922, Val Acc: 0.7613, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 194, Loss: 0.3813, Val Acc: 0.7733, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 195, Loss: 0.3755, Val Acc: 0.7613, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 196, Loss: 0.3778, Val Acc: 0.7667, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 197, Loss: 0.3735, Val Acc: 0.7693, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 198, Loss: 0.3749, Val Acc: 0.7667, Test Acc: 0.7613\n",
      "Seed: 44, Epoch: 199, Loss: 0.3746, Val Acc: 0.7653, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 200, Loss: 0.3754, Val Acc: 0.7600, Test Acc: 0.7547\n",
      "Average Time: 326.71 seconds\n",
      "Var Time: 28.30 seconds\n",
      "Average Memory: 10496.00 MB\n",
      "Average Best Val Acc: 0.7920\n",
      "Std Best Test Acc: 0.0146\n",
      "Average Test Acc: 0.7609\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data\"\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"COLLAB\", transform=T.Compose([T.OneHotDegree(491)]), use_node_attr=True)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time.time()\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "    torch.cuda.empty_cache()\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 2 --run_times=5 --patience=150 --epochs=500 --gsa_ratio=0.1 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 2 --run_times=5 --patience=150 --epochs=500 --gsa_ratio=0.3 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 2 --run_times=5 --patience=150 --epochs=500 --gsa_ratio=0.5 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 2 --run_times=5 --patience=150 --epochs=500 --gsa_ratio=0.7 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 2 --run_times=5 --patience=150 --epochs=500 --gsa_ratio=0.9 --pooling='GSA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.1 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.3 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.5 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.7 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.9 --pooling='GSA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freesolv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python -W ignore  /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.1 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python -W ignore  /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.3 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python -W ignore  /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.5 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python -W ignore  /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.7 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python -W ignore  /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.9 --pooling='GSA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipophilicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/XXX/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
